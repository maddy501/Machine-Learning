{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_Data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13pT9k8ks3ts"
      },
      "source": [
        "# **Exploring categorical features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDwFta48s5p-"
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read 'gapminder.csv' into a DataFrame: df\n",
        "df = pd.read_csv('gapminder.csv')\n",
        "\n",
        "# Create a boxplot of life expectancy per region\n",
        "df.boxplot('life','Region', rot=60)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvkA2-Nzs_cE"
      },
      "source": [
        "# **Creating dummy variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zle11w-jt2fa"
      },
      "source": [
        "Use the pandas get_dummies() function to create dummy variables from the df DataFrame. Store the result as df_region.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ywjgesEt4Xd"
      },
      "source": [
        "# Create dummy variables: df_region\n",
        "df_region = pd.get_dummies(df)\n",
        "\n",
        "# Print the columns of df_region\n",
        "print(df_region.columns)\n",
        "\n",
        "# Create dummy variables with drop_first=True: df_region\n",
        "df_region = pd.get_dummies(df,drop_first=True)\n",
        "\n",
        "# Print the new columns of df_region\n",
        "print(df_region.columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu8uNlyCuaef"
      },
      "source": [
        "# Regression with categorical **features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXRbd0ZiudPx"
      },
      "source": [
        "# Import necessary modules\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Instantiate a ridge regressor: ridge\n",
        "ridge = Ridge(alpha=0.5, normalize = True)\n",
        "\n",
        "# Perform 5-fold cross-validation: ridge_cv\n",
        "ridge_cv = cross_val_score(ridge,X,y,cv=5)\n",
        "\n",
        "# Print the cross-validated scores\n",
        "print(ridge_cv)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s86eB861wiXO"
      },
      "source": [
        "# **Dropping missing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oscFHDZwkrW"
      },
      "source": [
        "# Convert '?' to NaN\n",
        "df[df == '?'] = np.nan\n",
        "\n",
        "# Print the number of NaNs\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Print shape of original DataFrame\n",
        "print(\"Shape of Original DataFrame: {}\".format(df.shape))\n",
        "\n",
        "# Drop missing values and print shape of new DataFrame\n",
        "df = df.dropna()\n",
        "\n",
        "# Print shape of new DataFrame\n",
        "print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cniETU4HxIi6"
      },
      "source": [
        "# **Imputing missing data in a ML Pipeline I**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRjSvE1uxP8N"
      },
      "source": [
        "Import Imputer from sklearn.preprocessing and SVC from sklearn.svm. SVC stands for Support Vector Classification, which is a type of SVM.\n",
        "Setup the Imputation transformer to impute missing data (represented as 'NaN') with the 'most_frequent' value in the column (axis=0).\n",
        "Instantiate a SVC classifier. Store the result in clf.\n",
        "Create the steps of the pipeline by creating a list of tuples:\n",
        "The first tuple should consist of the imputation step, using imp.\n",
        "The second should consist of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3K3z7GAxQks"
      },
      "source": [
        "# Import the Imputer module\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Setup the Imputation transformer: imp\n",
        "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
        "\n",
        "# Instantiate the SVC classifier: clf\n",
        "clf = SVC()\n",
        "\n",
        "# Setup the pipeline with the required steps: steps\n",
        "steps = [('imputation', imp),\n",
        "        ('SVM', clf)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTNyXC7jyQ8d"
      },
      "source": [
        "# Imputing missing data in a ML Pipeline **II**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmxKcqfQyUBN"
      },
      "source": [
        "Import the following modules:\n",
        "Imputer from sklearn.preprocessing and Pipeline from sklearn.pipeline.\n",
        "SVC from sklearn.svm.\n",
        "Create the pipeline using Pipeline() and steps.\n",
        "Create training and test sets. Use 30% of the data for testing and a random state of 42.\n",
        "Fit the pipeline to the training set and predict the labels of the test set.\n",
        "Compute the classification report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdt5EVIlyXgs"
      },
      "source": [
        "# Import necessary modules\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Setup the pipeline steps: steps\n",
        "steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
        "        ('SVM', SVC())]\n",
        "\n",
        "# Create the pipeline: pipeline\n",
        "pipeline = Pipeline(steps = steps)\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
        "\n",
        "# Fit the pipeline to the train set\n",
        "pipeline.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Compute metrics\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjp5YIBH9V1J"
      },
      "source": [
        "# **Centering and scaling your data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXRSkNC99bxX"
      },
      "source": [
        "# Import scale\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "# Scale the features: X_scaled\n",
        "X_scaled = scale(X)\n",
        "\n",
        "# Print the mean and standard deviation of the unscaled features\n",
        "print(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \n",
        "print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n",
        "\n",
        "# Print the mean and standard deviation of the scaled features\n",
        "print(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \n",
        "print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c603o-FX-6pu"
      },
      "source": [
        "# **Centering and scaling in a pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLP4HyIW_AcX"
      },
      "source": [
        "With regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUTNn6mA_BOW"
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Setup the pipeline steps: steps\n",
        "steps = [('scaler', StandardScaler()),\n",
        "        ('knn', KNeighborsClassifier())]\n",
        "        \n",
        "# Create the pipeline: pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
        "\n",
        "# Fit the pipeline to the training set: knn_scaled\n",
        "knn_scaled = pipeline.fit(X_train,y_train)\n",
        "\n",
        "# Instantiate and fit a k-NN classifier to the unscaled data\n",
        "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
        "\n",
        "# Compute and print metrics\n",
        "print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test,y_test)))\n",
        "print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test,y_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aavwbxE5CLNY"
      },
      "source": [
        "# **Bringing it all together I: Pipeline for classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-W_XD32CUDn"
      },
      "source": [
        "Setup the pipeline with the following steps:\n",
        "Scaling, called 'scaler' with StandardScaler().\n",
        "Classification, called 'SVM' with SVC().\n",
        "Specify the hyperparameter space using the following notation: 'step_name__parameter_name'. Here, the step_name is SVM, and the parameter_names are C and gamma.\n",
        "Create training and test sets, with 20% of the data used for the test set. Use a random state of 21.\n",
        "Instantiate GridSearchCV with the pipeline and hyperparameter space and fit it to the training set. Use 3-fold cross-validation (This is the default, so you don't have to specify it).\n",
        "Predict the labels of the test set and compute the metrics. The metrics have been computed for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8tZh-YrCXPv"
      },
      "source": [
        "# Setup the pipeline\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('SVM', SVC())]\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Specify the hyperparameter space\n",
        "parameters = {'SVM__C':[1, 10, 100],\n",
        "              'SVM__gamma':[0.1, 0.01]}\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.2 ,random_state=21)\n",
        "\n",
        "# Instantiate the GridSearchCV object: cv\n",
        "cv = GridSearchCV(pipeline,parameters)\n",
        "\n",
        "# Fit to the training set\n",
        "cv.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred\n",
        "y_pred = cv.predict(X_test)\n",
        "\n",
        "# Compute and print metrics\n",
        "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7IqDkXnNS2A"
      },
      "source": [
        "# **Bringing it all together II: Pipeline for regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylacJn5jNUua"
      },
      "source": [
        "Your job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSr7t1AvNx2M"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "# Setup the pipeline steps: steps\n",
        "steps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
        "         ('scaler', StandardScaler()),\n",
        "         ('elasticnet', ElasticNet())]\n",
        "\n",
        "# Create the pipeline: pipeline \n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Specify the hyperparameter space\n",
        "parameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.4 , random_state=42)\n",
        "\n",
        "# Create the GridSearchCV object: gm_cv\n",
        "gm_cv = GridSearchCV(pipeline,parameters)\n",
        "\n",
        "# Fit to the training set\n",
        "gm_cv.fit(X_train,y_train)\n",
        "\n",
        "# Compute and print the metrics\n",
        "r2 = gm_cv.score(X_test, y_test)\n",
        "print(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n",
        "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}